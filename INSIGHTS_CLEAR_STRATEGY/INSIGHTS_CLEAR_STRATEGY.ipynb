{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "blhhx53yxze2gowqxfdj",
   "authorId": "3597297593686",
   "authorName": "SHREYAS",
   "authorEmail": "lengadeshreyas06work@gmail.com",
   "sessionId": "f64f0ddb-d40d-4c62-807b-049fc403f22f",
   "lastEditTime": 1740234022407
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "1fc33b5b-25a7-4681-9c8a-27a13152abc3",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.compose import ColumnTransformer  # Added missing import\nfrom imblearn.over_sampling import SMOTE\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom imblearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc04e830-4752-4f16-91fa-e4423d67585d",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "data = pd.read_csv(\"yds.csv\")\n# Quick overview of the data\nprint(\"Shape of dataset:\", data.shape)\nprint(data.head())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "865a23f0-9d27-4e54-b8b4-1da26478f6b8",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create a DataFrame with only rows where is_goal equals 1\ndf_goals = data[data['is_goal'] == 1].copy()\n\n# Verify the result\nprint(\"Shape of DataFrame where is_goal == 1:\", df_goals.shape)\nprint(df_goals.head())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1596a14f-de2a-47e9-a9e6-80d605735340",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# 3. Check missing values (count & %)\nmissing_data = pd.DataFrame({\n    'total_missing': data.isnull().sum(), \n    'perc_missing': (data.isnull().sum() / data.shape[0]) * 100\n})\nprint(missing_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "adccb9ee-cdbb-4158-a27d-c9ee4212c96e",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Filter rows where all three columns are missing\nmissing_all = data[data[['area_of_shot', 'shot_basics', 'range_of_shot']].isnull().all(axis=1)]\n\n# Print the number of rows and, if needed, inspect the rows\nprint(\"Number of rows with all three columns missing:\", len(missing_all))\nprint(missing_all)\n\n# Remove rows where all three columns are missing\ndata = data[~data[['area_of_shot', 'shot_basics', 'range_of_shot']].isna().all(axis=1)]\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eaacc358-e325-46f0-9ab5-731e678091b0",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "cols_to_check = ['area_of_shot', 'shot_basics', 'range_of_shot', 'is_goal']\ndata_complete = data.dropna(subset=cols_to_check, how='all')\n\nprint(\"Shape of dataset after removing rows where all these columns are missing:\", data_complete.shape)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a515c945-2bb8-491a-ab17-91eb94466608",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "print(\"Unique values in range_of_shot:\", data['range_of_shot'].unique())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8f6ccac-7d6f-43b6-89cc-67ef1914e90f",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "# -----------------------------------------------\n# 3. Football-Specific Feature Engineering \n# -----------------------------------------------\ndef create_football_features(df):\n    df = df.copy()\n    # Example: create a backcourt flag from range_of_shot (if applicable)\n    df['is_backcourt'] = df['range_of_shot'] == 'Back Court Shot'\n    # Optionally enforce domain rules (e.g., if backcourt then force a certain area/shot type)\n    df.loc[df['is_backcourt'], 'area_of_shot'] = 'Mid Ground (MG)'\n    df.loc[df['is_backcourt'], 'shot_basics'] = df.loc[df['is_backcourt'], 'shot_basics']\\\n        .where(df['shot_basics'].isin(['Mid Ground Line', 'Penalty Spot']), 'Mid Ground Line')\n    return df\n\ndata = create_football_features(data)\n\n# -----------------------------------------------\n# 4. Define Evaluation and Imputation Functions\n# -----------------------------------------------\ndef evaluate_imputation_model(df, target_col, predictor_cols, model, random_state=42, mask_fraction=0.1):\n    \"\"\"\n    Evaluate a model for imputing a categorical target column by masking a fraction\n    of complete data and computing accuracy.\n    \"\"\"\n    df_complete = df[df[target_col].notnull()].copy()\n    if df_complete.empty:\n        print(f\"No complete data available for {target_col}\")\n        return None\n    \n    le_target = LabelEncoder()\n    df_complete[target_col + '_enc'] = le_target.fit_transform(df_complete[target_col])\n    \n    for col in predictor_cols:\n        le = LabelEncoder()\n        df_complete[col] = df_complete[col].astype(str)\n        df_complete[col + '_enc'] = le.fit_transform(df_complete[col])\n    \n    features = [col + '_enc' for col in predictor_cols]\n    X = df_complete[features]\n    y = df_complete[target_col + '_enc']\n    \n    np.random.seed(random_state)\n    mask = np.random.rand(len(df_complete)) < mask_fraction\n    y_true = y[mask].copy()\n    \n    model.fit(X, y)\n    y_pred = model.predict(X[mask])\n    \n    acc = accuracy_score(y_true, y_pred)\n    print(f\"Model evaluation for {target_col} using {model.__class__.__name__}: Accuracy = {acc:.4f}\")\n    return acc\n\ndef impute_categorical_model(df, target_col, predictor_cols, model, random_state=42):\n    \"\"\"\n    Impute missing values in a target categorical column using a predictive model.\n    This version handles previously unseen labels in predictor columns by replacing them\n    with the mode from the training set.\n    \"\"\"\n    # Separate rows where target is present (for training) and where it's missing (for prediction)\n    train_df = df[df[target_col].notnull()].copy()\n    test_df = df[df[target_col].isnull()].copy()\n    \n    if test_df.empty:\n        print(f\"No missing values in {target_col} to impute using {model.__class__.__name__}.\")\n        return df\n    \n    # Encode the target column using LabelEncoder on training data\n    le_target = LabelEncoder()\n    train_df[target_col + '_enc'] = le_target.fit_transform(train_df[target_col])\n    \n    # For each predictor, encode training data and then process test data:\n    for col in predictor_cols:\n        le = LabelEncoder()\n        # Convert column to string (if not already)\n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        # Fit on training data\n        train_df[col + '_enc'] = le.fit_transform(train_df[col])\n        \n        # In test data, replace unseen values with the mode from training data\n        unseen_mask = ~test_df[col].isin(le.classes_)\n        if unseen_mask.any():\n            mode_val = train_df[col].mode()[0]\n            test_df.loc[unseen_mask, col] = mode_val\n        \n        # Transform test data\n        test_df[col + '_enc'] = le.transform(test_df[col])\n    \n    # Prepare training features and target\n    X_train = train_df[[col + '_enc' for col in predictor_cols]]\n    y_train = train_df[target_col + '_enc']\n    \n    # Fit the chosen model on the training data\n    model.fit(X_train, y_train)\n    \n    # Prepare test features and predict missing target values\n    X_test = test_df[[col + '_enc' for col in predictor_cols]]\n    y_pred_enc = model.predict(X_test)\n    y_pred = le_target.inverse_transform(y_pred_enc)\n    \n    # Fill the missing values in the original DataFrame\n    df.loc[df[target_col].isnull(), target_col] = y_pred\n    print(f\"Imputed missing values in '{target_col}' using {model.__class__.__name__}.\")\n    return df\n\n\n# -----------------------------------------------\n# 5. Set Up Target and Predictor Columns\n# -----------------------------------------------\n# For shot_basics and range_of_shot, we use predictors: area_of_shot and range_of_shot or shot_basics respectively.\n# For area_of_shot, we now include additional predictors 'location_x' and 'location_y' to improve accuracy.\ntarget_columns = ['area_of_shot', 'shot_basics', 'range_of_shot']\npredictor_columns = {\n    'area_of_shot': ['shot_basics', 'range_of_shot', 'location_x', 'location_y'],  # extra predictors added\n    'shot_basics': ['area_of_shot', 'range_of_shot'],\n    'range_of_shot': ['area_of_shot', 'shot_basics']\n}\n\n# -----------------------------------------------\n# 6. Evaluate Imputation Models for Each Target Column\n# -----------------------------------------------\nevaluation_results = {}\n# Initialize models\ndt_model = DecisionTreeClassifier(random_state=42)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\net_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n\nprint(\"\\nEvaluating imputation for target columns...\\n\")\nfor target in target_columns:\n    preds = predictor_columns[target]\n    if target == 'area_of_shot':\n        # Evaluate three models for area_of_shot\n        acc_dt = evaluate_imputation_model(data, target, preds, dt_model)\n        acc_rf = evaluate_imputation_model(data, target, preds, rf_model)\n        acc_et = evaluate_imputation_model(data, target, preds, et_model)\n        best_model_name = None\n        best_acc = max(acc_dt, acc_rf, acc_et)\n        if best_acc == acc_et:\n            best_model_name = \"ExtraTrees\"\n        elif best_acc == acc_rf:\n            best_model_name = \"RandomForest\"\n        else:\n            best_model_name = \"DecisionTree\"\n        evaluation_results[target] = {\"DecisionTree\": acc_dt, \"RandomForest\": acc_rf, \"ExtraTrees\": acc_et, \"Best\": best_model_name}\n    else:\n        # Evaluate for shot_basics and range_of_shot using DT and RF\n        acc_dt = evaluate_imputation_model(data, target, preds, dt_model)\n        acc_rf = evaluate_imputation_model(data, target, preds, rf_model)\n        best_model_name = \"DecisionTree\" if acc_dt >= acc_rf else \"RandomForest\"\n        evaluation_results[target] = {\"DecisionTree\": acc_dt, \"RandomForest\": acc_rf, \"Best\": best_model_name}\n\nprint(\"\\nEvaluation Results:\")\nprint(evaluation_results)\n\n# -----------------------------------------------\n# 7. Impute Missing Values Using the Best Model for Each Target Column\n# -----------------------------------------------\nfinal_imputed_data = data.copy()\nfor target in target_columns:\n    preds = predictor_columns[target]\n    if target == 'area_of_shot':\n        if evaluation_results[target][\"Best\"] == \"ExtraTrees\":\n            final_imputed_data = impute_categorical_model(final_imputed_data, target, preds, et_model)\n        elif evaluation_results[target][\"Best\"] == \"RandomForest\":\n            final_imputed_data = impute_categorical_model(final_imputed_data, target, preds, rf_model)\n        else:\n            final_imputed_data = impute_categorical_model(final_imputed_data, target, preds, dt_model)\n    else:\n        if evaluation_results[target][\"Best\"] == \"DecisionTree\":\n            final_imputed_data = impute_categorical_model(final_imputed_data, target, preds, dt_model)\n        else:\n            final_imputed_data = impute_categorical_model(final_imputed_data, target, preds, rf_model)\n\n# -----------------------------------------------\n# 8. Post-Imputation Validation\n# -----------------------------------------------\nprint(\"\\nMissing values after model-based imputation in target columns:\")\nprint(final_imputed_data[target_columns].isnull().sum())\n\n# For error analysis, we can optionally generate a classification report\n\ndef error_analysis(target_col):\n    \"\"\"\n    Generate classification report for the target column using a pipeline.\n    If any class has fewer than 6 samples, the SMOTE step is skipped.\n    Uses RandomForestClassifier instead of CatBoostClassifier.\n    \"\"\"\n    predictors = predictor_columns[target_col][:2]\n    df_complete = final_imputed_data.dropna(subset=[target_col]).copy()\n    X = df_complete[predictors]\n    y = df_complete[target_col]\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y, random_state=42)\n    \n    # Build pipeline steps conditionally\n    steps = [\n        ('encoder', ColumnTransformer([\n            ('enc', OneHotEncoder(handle_unknown='ignore'), predictors)\n        ], remainder='passthrough'))\n    ]\n    \n    # Check class sizes to determine if SMOTE is feasible\n    class_counts = y_train.value_counts()\n    if class_counts.min() < 6:\n        print(f\"Some classes in {target_col} have fewer than 6 samples; skipping SMOTE.\")\n    else:\n        steps.append(('smote', SMOTE(random_state=42)))\n    \n    steps.append(('classifier', RandomForestClassifier(n_estimators=100, random_state=42)))\n    \n    pipeline = Pipeline(steps)\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    \n    print(f\"\\n{target_col} Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    print(f\"Weighted F1 Score: {f1:.4f}\\n\")\n# Now run error analysis for each target column\nfor col in ['area_of_shot', 'shot_basics', 'range_of_shot']:\n    error_analysis(col)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcb6bf54-8516-4e13-b987-9771fe506e37",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "#remaining min,sec\ncols_to_plot = ['remaining_min', 'remaining_min.1', 'remaining_sec', 'remaining_sec.1']\nexisting_cols = [col for col in cols_to_plot if col in data.columns]\n\nfor col in existing_cols:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(y=data[col])\n    plt.title(f\"Boxplot of {col}\")\n    plt.ylabel(col)\n    plt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6d70bf9-b62f-4a41-8b1c-3464fc148d93",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Due to prsence of high number of outliers in remianing.min.1 and remaining.sec.1 whose values are skewed which could affect analysis from precision and accuracy POV,\n# we remove these columns\n# Define the duplicate columns to remove\ncols_to_drop = [\"remaining_min.1\",\"remaining_sec.1\",\"power_of_shot\",\"power_of_shot.1\",\"knockout_match.1\"]\n\n# Remove these columns from the DataFrame\ndata.drop(cols_to_drop, axis=1, inplace=True)\n\n# Optionally, check the remaining columns\nprint(\"Columns after removal:\")\nprint(data.columns.tolist())\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bbe25bd-d1fe-4982-8d3c-cbd02be49c8a",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Remove the 'is_backcourt' column from the DataFrame 'data'\ndata.drop('is_backcourt', axis=1, inplace=True)\n\n# Verify that the column has been removed\nprint(\"Columns after removal:\")\nprint(data.columns.tolist())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1b1a5bb-66ac-4299-86f2-c6cd76ee2b44",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "collapsed": false
   },
   "outputs": [],
   "source": "data.drop('distance_of_shot.1',axis=1,inplace=True)\n# Verify that the column has been removed\nprint(\"Columns after removal:\")\nprint(data.columns.tolist())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d3fca09-fabc-46d3-9a8c-f5c5d082419c",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_copy = data.copy()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "115c43c5-8617-4f52-897c-55018c48bd91",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7de23b6-0cd2-408e-835b-23bd80815c60",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "collapsed": false
   },
   "outputs": [],
   "source": "import snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3498b7b6-c42e-43c7-b97e-9e73b4ad79e7",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\n\n# 1. Obtain the active session in your Snowflake Notebook\nsession = Session.get_active_session()\n\n# 2. Suppose 'df_copy' is your final Pandas DataFrame\n# Convert it to a Snowpark DataFrame\nsnowpark_df = session.create_dataframe(df_copy)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48211ece-9788-48f5-8710-9a015d6a8825",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "collapsed": false
   },
   "outputs": [],
   "source": "# 1. Create or replace a temporary stage\nsession.sql(\"CREATE OR REPLACE STAGE my_temp_stage\").collect()\n\n# 2. Write the Snowpark DataFrame to the stage as a CSV file\nsnowpark_df.write.copy_into_location(\n    location=\"@my_temp_stage/mydata.csv\",\n    file_format_type=\"csv\",\n    header=True,\n    overwrite=True\n)\n\n# 3. Download the file from the stage to your local machine\nsession.file.get(\"@my_temp_stage/mydata.csv\", \"mydata.csv\")\n\nprint(\"File downloaded as 'mydata.csv'\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3549950f-cfff-45ce-a39a-32f678a3dc91",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Download the file from the stage to the /tmp folder\nsession.file.get(\"@my_temp_stage/mydata.csv\", \"/tmp/mydata.csv\")\nprint(\"File downloaded from stage to /tmp/mydata.csv\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "562cea47-749e-47fb-b7e8-043a4e6711a4",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "collapsed": false
   },
   "outputs": [],
   "source": "import os\n\n# List all files in the /tmp folder\nfiles_in_tmp = os.listdir('/tmp')\nprint(files_in_tmp)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16c452e8-dca1-4694-bae9-51b0a0636159",
   "metadata": {
    "language": "python",
    "name": "cell20",
    "collapsed": false
   },
   "outputs": [],
   "source": "from IPython.display import FileLink, display\n\n# Create a clickable download link for the specific file\ndownload_link = FileLink(r'/tmp/mydata.csv/mydata.csv_0_0_0.csv.gz')\ndisplay(download_link)\n",
   "execution_count": null
  }
 ]
}